{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MWZDkwv-jy1y"
      },
      "source": [
        "# Deep Q learning with Doom üïπÔ∏è\n",
        "In this notebook we'll implement an agent <b>that plays Doom by using a Deep Q learning architecture.</b> <br>\n",
        "Our agent playing Doom:\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"assets/doom.gif\" style=\"max-width: 600px;\" alt=\"Deep Q learning with Doom\"/>\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2dxPMGfgKkaq"
      },
      "source": [
        "## For Google Collab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjsRWCa9n9UM",
        "outputId": "88fdeea2-1f95-4561-bbed-799755fb5baa"
      },
      "outputs": [],
      "source": [
        "# %%bash\n",
        "# # Install deps from\n",
        "# # https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md#-linux\n",
        "\n",
        "# apt-get install build-essential zlib1g-dev libsdl2-dev libjpeg-dev \\\n",
        "# nasm tar libbz2-dev libgtk2.0-dev cmake git libfluidsynth-dev libgme-dev \\\n",
        "# libopenal-dev timidity libwildmidi-dev unzip\n",
        "\n",
        "# # Boost libraries\n",
        "# apt-get install libboost-all-dev\n",
        "\n",
        "# # Lua binding dependencies\n",
        "# apt-get install liblua5.1-dev"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qa4giPWrLKJZ"
      },
      "source": [
        "## Install vizdoom, tensorflow and scikit-image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hr_2wQnn_LN",
        "outputId": "cee142fe-9ee9-4365-a6e1-6ebb4f47efbb"
      },
      "outputs": [],
      "source": [
        "# !pip install vizdoom\n",
        "# !pip install tensorflow\n",
        "# !pip install scikit-image"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IS1O533Kjy13"
      },
      "source": [
        "## Step 1: Import the libraries üìö"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "collapsed": true,
        "id": "p46soGNyjy15"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf      # Deep Learning library\n",
        "import tensorflow.keras as keras      # Deep Learning library\n",
        "import numpy as np           # Handle matrices\n",
        "from vizdoom import *        # Doom Environment\n",
        "\n",
        "import random                # Handling random number generation\n",
        "import time                  # Handling time calculation\n",
        "from skimage import transform# Help us to preprocess the frames\n",
        "\n",
        "from collections import deque# Ordered collection with ends\n",
        "import matplotlib.pyplot as plt # Display graphs\n",
        "\n",
        "import warnings # This ignore all the warning messages that are normally printed during the training because of skiimage\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "13_P_Cr1jy16"
      },
      "source": [
        "## Step 2: Create our environment üéÆ\n",
        "- Now that we imported the libraries/dependencies, we will create our environment.\n",
        "- Doom environment takes:\n",
        "    - A `configuration file` that **handle all the options** (size of the frame, possible actions...)\n",
        "    - A `scenario file`: that **generates the correct scenario** (in our case basic **but you're invited to try other scenarios**).\n",
        "- Note: We have 3 possible actions `[[0,0,1], [1,0,0], [0,1,0]]` so we don't need to do one hot encoding (thanks to < a href=\"https://stackoverflow.com/users/2237916/silgon\">silgon</a> for figuring out.\n",
        "\n",
        "### Our environment\n",
        "<img src=\"assets/doom.png\" style=\"max-width:500px;\" alt=\"Doom\"/>\n",
        "\n",
        "- A monster is spawned **randomly somewhere along the opposite wall**.\n",
        "- Player can only go **left/right and shoot**.\n",
        "- 1 hit is enough **to kill the monster**.\n",
        "- Episode finishes when **monster is killed or on timeout (300)**.\n",
        "<br><br>\n",
        "REWARDS:\n",
        "\n",
        "- +101 for killing the monster\n",
        "- -5 for missing\n",
        "- Episode ends after killing the monster or on timeout.\n",
        "- living reward = -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "collapsed": true,
        "id": "pTJOxD5Ljy16"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Here we create our environment\n",
        "\"\"\"\n",
        "def create_environment():\n",
        "    game = DoomGame()\n",
        "\n",
        "    # Load the correct configuration\n",
        "    game.load_config(\"basic.cfg\")\n",
        "\n",
        "    # Load the correct scenario (in our case basic scenario)\n",
        "    game.set_doom_scenario_path(\"basic.wad\")\n",
        "\n",
        "    game.init()\n",
        "\n",
        "    # Here our possible actions\n",
        "    left = [1, 0, 0]\n",
        "    right = [0, 1, 0]\n",
        "    shoot = [0, 0, 1]\n",
        "    possible_actions = [left, right, shoot]\n",
        "\n",
        "    return game, possible_actions\n",
        "\n",
        "\"\"\"\n",
        "Here we performing random action to test the environment\n",
        "\"\"\"\n",
        "def test_environment():\n",
        "    game = DoomGame()\n",
        "    game.load_config(\"basic.cfg\")\n",
        "    game.set_doom_scenario_path(\"basic.wad\")\n",
        "    game.init()\n",
        "    shoot = [0, 0, 1]\n",
        "    left = [1, 0, 0]\n",
        "    right = [0, 1, 0]\n",
        "    actions = [shoot, left, right]\n",
        "\n",
        "    episodes = 10\n",
        "    for i in range(episodes):\n",
        "        game.new_episode()\n",
        "        while not game.is_episode_finished():\n",
        "            state = game.get_state()\n",
        "            img = state.screen_buffer\n",
        "            misc = state.game_variables\n",
        "            action = random.choice(actions)\n",
        "            print(action)\n",
        "            reward = game.make_action(action)\n",
        "            print (\"\\treward:\", reward)\n",
        "            time.sleep(0.02)\n",
        "        print (\"Result:\", game.get_total_reward())\n",
        "        time.sleep(2)\n",
        "    game.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "collapsed": true,
        "id": "3SipndqCjy17"
      },
      "outputs": [],
      "source": [
        "game, possible_actions = create_environment()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1Xms65Qmjy17"
      },
      "source": [
        "## Step 3: Define the preprocessing functions ‚öôÔ∏è\n",
        "### preprocess_frame\n",
        "Preprocessing is an important step, <b>because we want to reduce the complexity of our states to reduce the computation time needed for training.</b>\n",
        "<br><br>\n",
        "Our steps:\n",
        "- Grayscale each of our frames (because <b> color does not add important information </b>). But this is already done by the config file.\n",
        "- Crop the screen (in our case we remove the roof because it contains no information)\n",
        "- We normalize pixel values\n",
        "- Finally we resize the preprocessed frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "IMAGE_DIMS = (80, 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "collapsed": true,
        "id": "bTd0FuIdjy18"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "    preprocess_frame:\n",
        "    Take a frame.\n",
        "    Resize it.\n",
        "        __________________\n",
        "        |                 |\n",
        "        |                 |\n",
        "        |                 |\n",
        "        |                 |\n",
        "        |_________________|\n",
        "\n",
        "        to\n",
        "        _____________\n",
        "        |            |\n",
        "        |            |\n",
        "        |            |\n",
        "        |____________|\n",
        "    Normalize it.\n",
        "\n",
        "    return preprocessed_frame\n",
        "\n",
        "    \"\"\"\n",
        "def preprocess_frame(frame):\n",
        "    # Greyscale frame already done in our vizdoom config\n",
        "    # x = np.mean(frame,-1)\n",
        "\n",
        "    # Crop the screen (remove the roof because it contains no information)\n",
        "    # cropped_frame = frame[30:-10,30:-30]\n",
        "    cropped_frame = frame[20:-20, 20:-20]\n",
        "\n",
        "    # Normalize Pixel Values\n",
        "    normalized_frame = cropped_frame/255.0\n",
        "\n",
        "    # Resize\n",
        "    preprocessed_frame = transform.resize(normalized_frame, IMAGE_DIMS)\n",
        "    # plt.imshow(preprocessed_frame, interpolation='nearest')\n",
        "    # plt.show()\n",
        "\n",
        "    return preprocessed_frame"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FEoJcvmRjy18"
      },
      "source": [
        "### stack_frames\n",
        "üëè This part was made possible thanks to help of <a href=\"https://github.com/Miffyli\">Anssi</a><br>\n",
        "\n",
        "As explained in this really <a href=\"https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/\">  good article </a> we stack frames.\n",
        "\n",
        "Stacking frames is really important because it helps us to **give have a sense of motion to our Neural Network.**\n",
        "\n",
        "- First we preprocess frame\n",
        "- Then we append the frame to the deque that automatically **removes the oldest frame**\n",
        "- Finally we **build the stacked state**\n",
        "\n",
        "This is how work stack:\n",
        "- For the first frame, we feed 4 frames\n",
        "- At each timestep, **we add the new frame to deque and then we stack them to form a new stacked frame**\n",
        "- And so on\n",
        "<img src=\"https://raw.githubusercontent.com/simoninithomas/Deep_reinforcement_learning_Course/master/DQN/Space%20Invaders/assets/stack_frames.png\" alt=\"stack\">\n",
        "- If we're done, **we create a new stack with 4 new frames (because we are in a new episode)**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "collapsed": true,
        "id": "SIRVLOfWjy18"
      },
      "outputs": [],
      "source": [
        "STACK_SIZE = 4 # We stack 4 frames\n",
        "\n",
        "# Initialize deque with zero-images one array for each image\n",
        "stacked_frames  =  deque([np.zeros(IMAGE_DIMS, dtype=np.int) for i in range(STACK_SIZE)], maxlen=4)\n",
        "\n",
        "def stack_frames(stacked_frames, state, is_new_episode):\n",
        "    # Preprocess frame\n",
        "    frame = preprocess_frame(state)\n",
        "\n",
        "    if is_new_episode:\n",
        "        # Clear our stacked_frames\n",
        "        stacked_frames = deque([np.zeros(IMAGE_DIMS, dtype=np.int) for i in range(STACK_SIZE)], maxlen=4)\n",
        "\n",
        "        # Because we're in a new episode, copy the same frame 4x\n",
        "        stacked_frames.append(frame)\n",
        "        stacked_frames.append(frame)\n",
        "        stacked_frames.append(frame)\n",
        "        stacked_frames.append(frame)\n",
        "\n",
        "        # Stack the frames\n",
        "        stacked_state = np.stack(stacked_frames, axis=2)\n",
        "\n",
        "    else:\n",
        "        # Append frame to deque, automatically removes the oldest frame\n",
        "        stacked_frames.append(frame)\n",
        "\n",
        "        # Build the stacked state (first dimension specifies different frames)\n",
        "        stacked_state = np.stack(stacked_frames, axis=2)\n",
        "\n",
        "    return stacked_state, stacked_frames"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "H8Wr2Qcxjy19"
      },
      "source": [
        "## Step 4: Set up our hyperparameters ‚öóÔ∏è\n",
        "In this part we'll set up our different hyperparameters. But when you implement a Neural Network by yourself you will **not implement hyperparamaters at once but progressively**.\n",
        "\n",
        "- First, you begin by defining the neural networks hyperparameters when you implement the model.\n",
        "- Then, you'll add the training hyperparameters when you implement the training algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "collapsed": true,
        "id": "Oehg3_Gwjy19"
      },
      "outputs": [],
      "source": [
        "### MODEL HYPERPARAMETERS\n",
        "state_size = [*IMAGE_DIMS, STACK_SIZE]          # Our input is a stack of 4 frames hence 84x84x4 (Width, height, channels)\n",
        "action_size = game.get_available_buttons_size()              # 3 possible actions: left, right, shoot\n",
        "learning_rate =  0.00015        # Alpha (aka learning rate)\n",
        "\n",
        "### TRAINING HYPERPARAMETERS\n",
        "total_episodes = 1000           # Total episodes for training\n",
        "max_steps = 100                 # Max possible steps in an episode\n",
        "batch_size = 64\n",
        "\n",
        "# Exploration parameters for epsilon greedy strategy\n",
        "explore_start = 1.0             # exploration probability at start\n",
        "explore_stop = 0.15             # minimum exploration probability\n",
        "decay_rate = 0.000125           # exponential decay rate for exploration prob\n",
        "\n",
        "# Q learning hyperparameters\n",
        "gamma = 0.95                    # Discounting rate\n",
        "\n",
        "### MEMORY HYPERPARAMETERS\n",
        "pretrain_length = batch_size    # Number of experiences stored in the Memory when initialized for the first time\n",
        "memory_size = 1000000           # Number of experiences the Memory can keep\n",
        "\n",
        "### MODIFY THIS TO FALSE IF YOU JUST WANT TO SEE THE TRAINED AGENT\n",
        "training = False\n",
        "checkpoint_path = \"./models/model3/model.ckpt\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dM_mJtzYjy1-"
      },
      "source": [
        "## Step 5: Create our Deep Q-learning Neural Network model üß†\n",
        "<img src=\"assets/model.png\" alt=\"Model\" />\n",
        "This is our Deep Q-learning model:\n",
        "- We take a stack of 4 frames as input\n",
        "- It passes through 3 convnets\n",
        "- Then it is flatened\n",
        "- Finally it passes through 2 FC layers\n",
        "- It outputs a Q value for each actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "collapsed": true,
        "id": "ohiE6DlKjy1-"
      },
      "outputs": [],
      "source": [
        "class DQNetwork:\n",
        "\tdef __init__(self, state_size, action_size, learning_rate, name='DQNetwork'):\n",
        "\t\tself.state_size = state_size\n",
        "\t\tself.action_size = action_size\n",
        "\t\tself.learning_rate = learning_rate\n",
        "\n",
        "\t\twith tf.compat.v1.variable_scope(name):\n",
        "\t\t\n",
        "\t\t\t# We create the placeholders\n",
        "\t\t\t# *state_size means that we take each elements of state_size in tuple hence is like if we wrote\n",
        "\t\t\t# [None, IMAGE_SIZE.x, IMAGE_SIZE.y, 4]\n",
        "\t\t\tself.inputs_ = tf.compat.v1.placeholder(tf.float32, [None, *state_size], name=\"inputs\")\n",
        "\t\t\tself.actions_ = tf.compat.v1.placeholder(tf.float32, [None, 3], name=\"actions_\")\n",
        "\n",
        "\t\t\t# Remember that target_Q is the R(s,a) + ymax Qhat(s', a')\n",
        "\t\t\tself.target_Q = tf.compat.v1.placeholder(tf.float32, [None], name=\"target\")\n",
        "\n",
        "\t\t\tx = self.inputs_\n",
        "\n",
        "\t\t\tx = keras.layers.Conv2D(\n",
        "\t\t\t\tinput_shape=(None, *state_size),\n",
        "\t\t\t\tfilters = 32,\n",
        "\t\t\t\tstrides = 2,\n",
        "\t\t\t\tkernel_size = (8, 8),\n",
        "\t\t\t\tactivation = 'relu'\n",
        "\t\t\t)(x)\n",
        "\t\t\t\n",
        "\t\t\tx = keras.layers.MaxPooling2D(\n",
        "\t\t\t\tpool_size=(2, 2)\n",
        "\t\t\t)(x)\n",
        "\n",
        "\t\t\tx = keras.layers.Conv2D(\n",
        "\t\t\t\tinput_shape=(None, *state_size),\n",
        "\t\t\t\tfilters = 64,\n",
        "\t\t\t\tstrides = 2,\n",
        "\t\t\t\tkernel_size = (7, 7),\n",
        "\t\t\t\tactivation = 'relu'\n",
        "\t\t\t)(x)\n",
        "\t\t\t\n",
        "\t\t\tx = keras.layers.MaxPooling2D(\n",
        "\t\t\t\tpool_size=(2, 2)\n",
        "\t\t\t)(x)\n",
        "\n",
        "\t\t\tx = keras.layers.Flatten()(x)\n",
        "\n",
        "\t\t\tx = keras.layers.Dense(\n",
        "\t\t\t\tunits = 128,\n",
        "\t\t\t\tactivation = 'relu'\n",
        "\t\t\t)(x)\n",
        "\t\t\t\n",
        "\t\t\tself.output = keras.layers.Dense(\n",
        "\t\t\t\tunits = action_size\n",
        "\t\t\t)(x)\n",
        "\n",
        "\t\t\t# Q is our predicted Q value.\n",
        "\t\t\tself.Q = tf.reduce_sum(tf.multiply(self.output, self.actions_), axis=1)\n",
        "\n",
        "\n",
        "\t\t\t# The loss is the difference between our predicted Q_values and the Q_target\n",
        "\t\t\t# Sum(Qtarget - Q)^2\n",
        "\t\t\tself.loss = tf.reduce_mean(tf.square(self.target_Q - self.Q))\n",
        "\n",
        "\t\t\tself.optimizer = tf.compat.v1.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "equMspOFjy1_"
      },
      "outputs": [],
      "source": [
        "tf.compat.v1.disable_eager_execution()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "collapsed": true,
        "id": "BhBN0iVQjy1_"
      },
      "outputs": [],
      "source": [
        "# Reset the graph\n",
        "tf.compat.v1.reset_default_graph()\n",
        "\n",
        "# Instantiate the DQNetwork\n",
        "DQNetwork = DQNetwork(state_size, action_size, learning_rate)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "M_anr1wgjy2A"
      },
      "source": [
        "## Step 6: Experience Replay üîÅ\n",
        "Now that we create our Neural Network, **we need to implement the Experience Replay method.** <br><br>\n",
        "Here we'll create the Memory object that creates a deque.A deque (double ended queue) is a data type that **removes the oldest element each time that you add a new element.**\n",
        "\n",
        "This part was taken from Udacity : <a href=\"https://github.com/udacity/deep-learning/blob/master/reinforcement/Q-learning-cart.ipynb\" Cartpole DQN</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "collapsed": true,
        "id": "p3aX1W82jy2A"
      },
      "outputs": [],
      "source": [
        "class Memory():\n",
        "    def __init__(self, max_size):\n",
        "        self.buffer = deque(maxlen = max_size)\n",
        "\n",
        "    def add(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        buffer_size = len(self.buffer)\n",
        "        index = np.random.choice(np.arange(buffer_size),\n",
        "                                size = batch_size,\n",
        "                                replace = False)\n",
        "\n",
        "        return [self.buffer[i] for i in index]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bZCTmVC-jy2B"
      },
      "source": [
        "Here we'll **deal with the empty memory problem**: we pre-populate our memory by taking random actions and storing the experience (state, action, reward, new_state)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "collapsed": true,
        "id": "k1yKOKbVjy2B"
      },
      "outputs": [],
      "source": [
        "# Instantiate memory\n",
        "memory = Memory(max_size = memory_size)\n",
        "\n",
        "# Render the environment\n",
        "game.new_episode()\n",
        "\n",
        "for i in range(pretrain_length):\n",
        "    # If it's the first step\n",
        "    if i == 0:\n",
        "        # First we need a state\n",
        "        state = game.get_state().screen_buffer\n",
        "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
        "\n",
        "    # Random action\n",
        "    action = random.choice(possible_actions)\n",
        "\n",
        "    # Get the rewards\n",
        "    reward = game.make_action(action)\n",
        "\n",
        "    # Look if the episode is finished\n",
        "    done = game.is_episode_finished()\n",
        "\n",
        "    # If we're dead\n",
        "    if done:\n",
        "        # We finished the episode\n",
        "        next_state = np.zeros(state.shape)\n",
        "\n",
        "        # Add experience to memory\n",
        "        memory.add((state, action, reward, next_state, done))\n",
        "\n",
        "        # Start a new episode\n",
        "        game.new_episode()\n",
        "\n",
        "        # First we need a state\n",
        "        state = game.get_state().screen_buffer\n",
        "\n",
        "        # Stack the frames\n",
        "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
        "\n",
        "    else:\n",
        "        # Get the next state\n",
        "        next_state = game.get_state().screen_buffer\n",
        "        next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
        "\n",
        "        # Add experience to memory\n",
        "        memory.add((state, action, reward, next_state, done))\n",
        "\n",
        "        # Our state is now the next_state\n",
        "        state = next_state"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TxosCT1ajy2C"
      },
      "source": [
        "## Step 7: Set up Tensorboard üìä\n",
        "For more information about tensorboard, please watch this <a href=\"https://www.youtube.com/embed/eBbEDRsCmv4\">excellent 30min tutorial</a> <br><br>\n",
        "To launch tensorboard : `tensorboard --logdir=/tensorboard/dqn/1`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "collapsed": true,
        "id": "dxv-FiqWjy2C"
      },
      "outputs": [],
      "source": [
        "# Setup TensorBoard Writer\n",
        "writer = tf.compat.v1.summary.FileWriter(\"/tensorboard/dqn/1\")\n",
        "\n",
        "## Losses\n",
        "tf.compat.v1.summary.scalar(\"Loss\", DQNetwork.loss)\n",
        "\n",
        "write_op = tf.compat.v1.summary.merge_all()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vXR8b9TEjy2C"
      },
      "source": [
        "## Step 8: Train our Agent üèÉ‚Äç‚ôÇÔ∏è\n",
        "\n",
        "Our algorithm:\n",
        "<br>\n",
        "* Initialize the weights\n",
        "* Init the environment\n",
        "* Initialize the decay rate (that will use to reduce epsilon)\n",
        "<br><br>\n",
        "* **For** episode to max_episode **do**\n",
        "    * Make new episode\n",
        "    * Set step to 0\n",
        "    * Observe the first state $s_0$\n",
        "    <br><br>\n",
        "    * **While** step < max_steps **do**:\n",
        "        * Increase decay_rate\n",
        "        * With $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\mathrm{argmax}_a Q(s_t,a)$\n",
        "        * Execute action $a_t$ in simulator and observe reward $r_{t+1}$ and new state $s_{t+1}$\n",
        "        * Store transition $<s_t, a_t, r_{t+1}, s_{t+1}>$ in memory $D$\n",
        "        * Sample random mini-batch from $D$: $<s, a, r, s'>$\n",
        "        * Set $\\hat{Q} = r$ if the episode ends at $+1$, otherwise set $\\hat{Q} = r + \\gamma \\max_{a'}{Q(s', a')}$\n",
        "        * Make a gradient descent step with loss $(\\hat{Q} - Q(s, a))^2$\n",
        "    * **endfor**\n",
        "    <br><br>\n",
        "* **endfor**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "collapsed": true,
        "id": "UCO_qh3Qjy2D"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This function will do the part\n",
        "With √è¬µ select a random action atat, otherwise select at=argmaxaQ(st,a)\n",
        "\"\"\"\n",
        "def predict_action(explore_start, explore_stop, decay_rate, decay_step, state, actions):\n",
        "    ## EPSILON GREEDY STRATEGY\n",
        "    # Choose action a from state s using epsilon greedy.\n",
        "    ## First we randomize a number\n",
        "    exp_exp_tradeoff = np.random.rand()\n",
        "\n",
        "    # Here we'll use an improved version of our epsilon greedy strategy used in Q-learning notebook\n",
        "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
        "\n",
        "    if (explore_probability > exp_exp_tradeoff):\n",
        "        # Make a random action (exploration)\n",
        "        action = random.choice(possible_actions)\n",
        "\n",
        "    else:\n",
        "        # Get action from Q-network (exploitation)\n",
        "        # Estimate the Qs values state\n",
        "        Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
        "\n",
        "        # Take the biggest Q value (= the best action)\n",
        "        choice = np.argmax(Qs)\n",
        "        action = possible_actions[int(choice)]\n",
        "\n",
        "    return action, explore_probability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxTtUgyJjy2D",
        "outputId": "cba340cd-c250-4c1d-816f-f0cec7713e31"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "num_threads = 5\n",
        "os.environ[\"OMP_NUM_THREADS\"] = \"5\"\n",
        "os.environ[\"TF_NUM_INTRAOP_THREADS\"] = \"5\"\n",
        "os.environ[\"TF_NUM_INTEROP_THREADS\"] = \"5\"\n",
        "tf.compat.v1.config.threading.set_inter_op_parallelism_threads(num_threads)\n",
        "tf.compat.v1.config.threading.set_intra_op_parallelism_threads(num_threads)\n",
        "tf.compat.v1.config.set_soft_device_placement(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Saver will help us to save our model\n",
        "saver = tf.compat.v1.train.Saver()\n",
        "\n",
        "if training == True:\n",
        "    with tf.compat.v1.Session() as sess:\n",
        "        saver.restore(sess, checkpoint_path)\n",
        "        \n",
        "        # Initialize the variables\n",
        "        sess.run(tf.compat.v1.global_variables_initializer())\n",
        "        \n",
        "        # Initialize the decay rate (that will use to reduce epsilon) \n",
        "        decay_step = 0\n",
        "\n",
        "        # Init the game\n",
        "        game.init()\n",
        "\n",
        "        for episode in range(total_episodes):\n",
        "            # Set step to 0\n",
        "            step = 0\n",
        "            \n",
        "            # Initialize the rewards of the episode\n",
        "            episode_rewards = []\n",
        "            \n",
        "            # Make a new episode and observe the first state\n",
        "            game.new_episode()\n",
        "            state = game.get_state().screen_buffer\n",
        "            \n",
        "            # Remember that stack frame function also call our preprocess function.\n",
        "            state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
        "\n",
        "            while step < max_steps:\n",
        "                step += 1\n",
        "                \n",
        "                # Increase decay_step\n",
        "                decay_step +=1\n",
        "                \n",
        "                # Predict the action to take and take it\n",
        "                action, explore_probability = predict_action(explore_start, explore_stop, decay_rate, decay_step, state, possible_actions)\n",
        "\n",
        "                # Do the action\n",
        "                reward = game.make_action(action)\n",
        "\n",
        "                # Look if the episode is finished\n",
        "                done = game.is_episode_finished()\n",
        "                \n",
        "                # Add the reward to total reward\n",
        "                episode_rewards.append(reward)\n",
        "\n",
        "                # If the game is finished\n",
        "                if done:\n",
        "                    # the episode ends so no next state\n",
        "                    next_state = np.zeros((84,84), dtype=np.int)\n",
        "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
        "\n",
        "                    # Set step = max_steps to end the episode\n",
        "                    step = max_steps\n",
        "\n",
        "                    # Get the total reward of the episode\n",
        "                    total_reward = np.sum(episode_rewards)\n",
        "\n",
        "                    print('Episode: {}'.format(episode),\n",
        "                              'Total reward: {}'.format(total_reward),\n",
        "                              'Training loss: {:.4f}'.format(loss),\n",
        "                              'Explore P: {:.4f}'.format(explore_probability))\n",
        "\n",
        "                    memory.add((state, action, reward, next_state, done))\n",
        "\n",
        "                else:\n",
        "                    # Get the next state\n",
        "                    next_state = game.get_state().screen_buffer\n",
        "                    \n",
        "                    # Stack the frame of the next_state\n",
        "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
        "                    \n",
        "\n",
        "                    # Add experience to memory\n",
        "                    memory.add((state, action, reward, next_state, done))\n",
        "                    \n",
        "                    # st+1 is now our current state\n",
        "                    state = next_state\n",
        "\n",
        "\n",
        "                ### LEARNING PART            \n",
        "                # Obtain random mini-batch from memory\n",
        "                batch = memory.sample(batch_size)\n",
        "                states_mb = np.array([each[0] for each in batch], ndmin=3)\n",
        "                actions_mb = np.array([each[1] for each in batch])\n",
        "                rewards_mb = np.array([each[2] for each in batch]) \n",
        "                next_states_mb = np.array([each[3] for each in batch], ndmin=3)\n",
        "                dones_mb = np.array([each[4] for each in batch])\n",
        "\n",
        "                target_Qs_batch = []\n",
        "\n",
        "                 # Get Q values for next_state \n",
        "                Qs_next_state = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: next_states_mb})\n",
        "                \n",
        "                # Set Q_target = r if the episode ends at s+1, otherwise set Q_target = r + gamma*maxQ(s', a')\n",
        "                for i in range(0, len(batch)):\n",
        "                    terminal = dones_mb[i]\n",
        "\n",
        "                    # If we are in a terminal state, only equals reward\n",
        "                    if terminal:\n",
        "                        target_Qs_batch.append(rewards_mb[i])\n",
        "                        \n",
        "                    else:\n",
        "                        target = rewards_mb[i] + gamma * np.max(Qs_next_state[i])\n",
        "                        target_Qs_batch.append(target)\n",
        "                        \n",
        "\n",
        "                targets_mb = np.array([each for each in target_Qs_batch])\n",
        "\n",
        "                loss, _ = sess.run([DQNetwork.loss, DQNetwork.optimizer],\n",
        "                                    feed_dict={DQNetwork.inputs_: states_mb,\n",
        "                                               DQNetwork.target_Q: targets_mb,\n",
        "                                               DQNetwork.actions_: actions_mb})\n",
        "\n",
        "                # Write TF Summaries\n",
        "                summary = sess.run(write_op, feed_dict={DQNetwork.inputs_: states_mb,\n",
        "                                                   DQNetwork.target_Q: targets_mb,\n",
        "                                                   DQNetwork.actions_: actions_mb})\n",
        "                writer.add_summary(summary, episode)\n",
        "                writer.flush()\n",
        "\n",
        "            # Save model every 10 episodes\n",
        "            if (episode + 1) % 10 == 0:\n",
        "                save_path = saver.save(sess, checkpoint_path)\n",
        "                print(\"Model Saved\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "v9CaMf1ojy2E"
      },
      "source": [
        "## Step 9: Watch our Agent play üëÄ\n",
        "Now that we trained our agent, we can test it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "collapsed": true,
        "id": "h_qVzNR_jy2E"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./models/model3/model.ckpt\n",
            "Score:  94.0\n",
            "Score:  94.0\n",
            "Score:  94.0\n",
            "Score:  83.0\n",
            "Score:  94.0\n",
            "Score:  71.0\n",
            "Score:  94.0\n",
            "Score:  90.0\n",
            "Score:  63.0\n",
            "Score:  88.0\n",
            "Score:  61.0\n",
            "Score:  92.0\n",
            "Score:  44.0\n",
            "Score:  83.0\n",
            "Score:  94.0\n",
            "Score:  62.0\n",
            "Score:  94.0\n",
            "Score:  94.0\n",
            "Score:  94.0\n",
            "Score:  89.0\n",
            "Score:  63.0\n",
            "Score:  94.0\n",
            "Score:  94.0\n",
            "Score:  88.0\n",
            "Score:  69.0\n",
            "Score:  94.0\n",
            "Score:  94.0\n",
            "Score:  53.0\n",
            "Score:  94.0\n",
            "Score:  94.0\n",
            "Score:  94.0\n",
            "Score:  94.0\n",
            "Score:  70.0\n",
            "Score:  94.0\n",
            "Score:  94.0\n",
            "Score:  63.0\n",
            "Score:  94.0\n",
            "Score:  64.0\n",
            "Score:  94.0\n",
            "Score:  94.0\n",
            "Score:  94.0\n",
            "Score:  94.0\n",
            "Score:  94.0\n",
            "Score:  83.0\n",
            "Score:  88.0\n",
            "Score:  94.0\n",
            "Score:  94.0\n",
            "Score:  94.0\n",
            "Score:  94.0\n",
            "Score:  94.0\n",
            "Score:  94.0\n",
            "Score:  44.0\n",
            "Score:  94.0\n",
            "Score:  47.0\n",
            "Score:  94.0\n",
            "Score:  70.0\n",
            "Score:  88.0\n",
            "Score:  94.0\n",
            "Score:  94.0\n",
            "Score:  88.0\n",
            "Score:  94.0\n",
            "Score:  63.0\n",
            "Score:  94.0\n",
            "Score:  94.0\n",
            "Score:  88.0\n",
            "Score:  94.0\n",
            "Score:  50.0\n",
            "Score:  94.0\n",
            "Score:  49.0\n",
            "Score:  69.0\n",
            "Score:  69.0\n",
            "Score:  94.0\n",
            "Score:  94.0\n",
            "Score:  94.0\n",
            "Score:  62.0\n",
            "Score:  88.0\n",
            "Score:  66.0\n",
            "Score:  94.0\n",
            "Score:  94.0\n",
            "Score:  94.0\n",
            "Score:  94.0\n",
            "Score:  94.0\n",
            "Score:  57.0\n",
            "Score:  63.0\n",
            "Score:  54.0\n",
            "Score:  94.0\n",
            "Score:  42.0\n",
            "Score:  88.0\n",
            "Score:  64.0\n",
            "Score:  54.0\n",
            "Score:  64.0\n",
            "Score:  94.0\n",
            "Score:  70.0\n",
            "Score:  94.0\n",
            "Score:  94.0\n",
            "Score:  88.0\n",
            "Score:  83.0\n",
            "Score:  55.0\n",
            "Score:  94.0\n",
            "Score:  62.0\n"
          ]
        }
      ],
      "source": [
        "num_test = 100\n",
        "scores = []\n",
        "\n",
        "with tf.compat.v1.Session() as sess:\n",
        "    game, possible_actions = create_environment()\n",
        "\n",
        "    totalScore = 0\n",
        "\n",
        "    # Load the model\n",
        "    saver.restore(sess, checkpoint_path)\n",
        "    game.init()\n",
        "    for i in range(num_test):\n",
        "\n",
        "        done = False\n",
        "\n",
        "        game.new_episode()\n",
        "\n",
        "        state = game.get_state().screen_buffer\n",
        "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
        "\n",
        "        while not game.is_episode_finished():\n",
        "            # Take the biggest Q value (= the best action)\n",
        "            Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
        "\n",
        "            # Take the biggest Q value (= the best action)\n",
        "            choice = np.argmax(Qs)\n",
        "            action = possible_actions[int(choice)]\n",
        "\n",
        "            game.make_action(action)\n",
        "            done = game.is_episode_finished()\n",
        "            score = game.get_total_reward()\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "            else:\n",
        "                next_state = game.get_state().screen_buffer\n",
        "                next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
        "                state = next_state\n",
        "\n",
        "        score = game.get_total_reward()\n",
        "        scores.append(score)\n",
        "        print(\"Score: \", score)\n",
        "    game.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average:\t 82.34\n",
            "Min Score:\t 42.0\n",
            "Max Score:\t 94.0\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#create histogram\n",
        "plt.hist(scores, ec='black', bins=20)\n",
        "plt.xlabel(\"Score\")\n",
        "plt.ylabel(\"Count\")\n",
        "\n",
        "average =  sum(scores) / len(scores)\n",
        "min_score = min(scores)\n",
        "max_score = max(scores)\n",
        "print(\"Average:\\t\", average)\n",
        "print(\"Min Score:\\t\", min_score)\n",
        "print(\"Max Score:\\t\", max_score)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
